import torch
import json
import os
import trimesh
import numpy as np


def load_smpl_info(path2param, path2mesh=None):
    with open(path2param, 'r') as f:
        smpl_params = json.load(f)
    for key in smpl_params.keys():
        smpl_params[key] = torch.FloatTensor(smpl_params[key])[0].reshape(1, -1)

    if path2mesh is not None:
        smpl_mesh = trimesh.load(path2mesh, process=False)
        return smpl_params, smpl_mesh
    else:
        return smpl_params


def init_semantic_labels(path2label):
    """
    Set semantic labels for SMPL(-X) vertices
    :param path2label: path to the semantic information (json file)
    :results are saved in instance variables
    """
    # semantic labels for smplx vertices.
    if os.path.isfile(path2label):
        left_wrist_idx, right_wrist_idx = [], []
        hand_idx, non_hand_idx = [], []
        with open(path2label, "r") as json_file:
            v_label = json.load(json_file)
            v_label['leftWrist'], v_label['rightWrist'] = [], []
            v_label['hand_idx'], v_label['non_hand_idx'] = [], []
            v_label['left_wrist_idx'], v_label['right_wrist_idx'] = [], []

            for k in v_label['leftHand']:
                if k in v_label['leftForeArm']:
                    v_label['left_wrist_idx'].append(k)
            for k in v_label['rightHand']:
                if k in v_label['rightForeArm']:
                    v_label['right_wrist_idx'].append(k)
            for key in v_label.keys():
                if 'leftHand' in key or 'rightHand' in key:
                    v_label['hand_idx'].append(v_label[key])
                else:
                    v_label['non_hand_idx'].append(v_label[key])

            nonbody_idx = v_label['head'] + v_label['eyeballs'] + \
                               v_label['leftToeBase'] + v_label['rightToeBase'] + \
                               v_label['leftEye'] + v_label['rightEye'] + \
                               hand_idx
            nonbody_idx = np.asarray(list(set(nonbody_idx)))  # unique idx
            body_idx = np.asarray([i for i in range(0, 10475) if i not in nonbody_idx])
            v_label['body_idx'] = body_idx
            v_label['non_body_idx'] = nonbody_idx
        return v_label


def get_near_and_far_points(keypoints, mesh, K, R, t, res=512, visualize=False):
    """
        Sample code for retrieve 3D joints from reconstructed human model.
        :param mesh: reconstructed mesh w.r.t. the camera center (origin)
        :param scene: openGL scene generated by the cam_params
        :param keypoints: SMPL-X keypoints converted from openpose or openpifpaf keypoints
        :param cam_params: camera parameters (intrinsic and extrinsic)
        :return: 144 x 3 matrix that contains 3D joints of 144 landmarks
    """
    # 0. update scene to the predefined camera parameters.
    if callable(mesh.scene):
        scene = mesh.scene()
    else:
        scene = mesh.scene

    K = K.detach().cpu().numpy()
    t = t.detach().cpu().numpy()
    keypoints = keypoints.detach().cpu().numpy()
    scene.camera.focal = [K[0, 0], K[1, 1]]
    scene.camera.resolution = [res, res]
    scene.camera.principal_x = K[0, 2]
    scene.camera.principal_y = K[1, 2]

    cam_trans = np.copy(scene.camera_transform)
    cam_trans[:3, 3] = np.asarray(t)

    near = 1.0
    far = 5.0
    scene.camera_transform = cam_trans
    scene.camera.z_far = far
    scene.camera.z_near = near
    mesh.scene = scene

    # 1. make unit rays (z direction is negative)
    # xyz = np.ones_like(keypoints[:, 0:3])
    xyz = np.ones((144, 3))
    xyz[:, 0] = (keypoints[:, 0] - K[0, 2]) / K[0, 0]
    xyz[:, 1] = (keypoints[:, 1] - K[1, 2]) / K[1, 1]
    xyz = -xyz / np.linalg.norm(xyz, axis=1, keepdims=True)
    xyz[:, 0] = -xyz[:, 0]
    origins = np.tile(t, (keypoints.shape[0], 1))

    # 2. get depth from intersections
    pers_points, pers_index_ray, pers_index_tri = mesh.ray.intersects_location(origins,
                                                                               xyz,
                                                                               multiple_hits=True)
    depth = trimesh.util.diagonal_dot(pers_points - origins[0].reshape(1, 3), xyz[pers_index_ray, :])

    depth_near = np.ones_like(keypoints[:, 0:1], dtype=float) * scene.camera.z_far
    depth_far = np.zeros_like(keypoints[:, 0:1], dtype=float)

    for i in range(len(pers_index_ray)):
        depth_near[pers_index_ray[i]] = min(depth_near[pers_index_ray[i]], depth[i])
        depth_far[pers_index_ray[i]] = max(depth_far[pers_index_ray[i]], depth[i])

    xyz_near, xyz_far = np.zeros_like(xyz), np.zeros_like(xyz)
    xyz_near[pers_index_ray, :] = xyz[pers_index_ray, :] * depth_near[pers_index_ray, :] + origins[0].reshape(-1, 3)
    xyz_far[pers_index_ray, :] = xyz[pers_index_ray, :] * depth_far[pers_index_ray, :] + origins[0].reshape(-1, 3)

    return xyz_near, xyz_far


def get_3d_from_2d_keypoints(keypoints, mesh, K, R, t, res=512, visualize=False):
    """
        Sample code for retrieve 3D joints from reconstructed human model.
        :param mesh: reconstructed mesh w.r.t. the camera center (origin)
        :param scene: openGL scene generated by the cam_params
        :param keypoints: SMPL-X keypoints converted from openpose or openpifpaf keypoints
        :param cam_params: camera parameters (intrinsic and extrinsic)
        :return: 144 x 3 matrix that contains 3D joints of 144 landmarks
    """
    # 0. update scene to the predefined camera parameters.
    scene = mesh.scene()
    K = K.detach().cpu().numpy()
    t = t.detach().cpu().numpy()
    keypoints = keypoints.detach().cpu().numpy()
    scene.camera.focal = [K[0, 0], K[1, 1]]
    scene.camera.resolution = [res, res]
    scene.camera.principal_x = K[0, 2]
    scene.camera.principal_y = K[1, 2]

    cam_trans = np.copy(scene.camera_transform)
    cam_trans[:3, 3] = np.asarray(t)

    near = 1.0
    far = 5.0
    scene.camera_transform = cam_trans
    scene.camera.z_far = far
    scene.camera.z_near = near
    mesh.scene = scene

    # 1. make unit rays (z direction is negative)
    xyz = np.ones_like(keypoints[:, 0:3])
    xyz[:, 0] = (keypoints[:, 0] - K[0, 2]) / K[0, 0]
    xyz[:, 1] = (keypoints[:, 1] - K[1, 2]) / K[1, 1]
    xyz = -xyz / np.linalg.norm(xyz, axis=1, keepdims=True)
    xyz[:, 0] = -xyz[:, 0]
    origins = np.tile(t, (keypoints.shape[0], 1))

    # 2. get depth from intersections
    pers_points, pers_index_ray, pers_index_tri = mesh.ray.intersects_location(origins,
                                                                               xyz,
                                                                               multiple_hits=True)
    depth = trimesh.util.diagonal_dot(pers_points - origins[0].reshape(1, 3), xyz[pers_index_ray, :])

    depth_near = np.ones_like(keypoints[:, 0:1], dtype=float) * scene.camera.z_far
    depth_far = np.zeros_like(keypoints[:, 0:1], dtype=float)

    for i in range(len(pers_index_ray)):
        depth_near[pers_index_ray[i]] = min(depth_near[pers_index_ray[i]], depth[i])
        depth_far[pers_index_ray[i]] = max(depth_far[pers_index_ray[i]], depth[i])

    xyz_near, xyz_far = np.zeros_like(xyz), np.zeros_like(xyz)
    xyz_near[pers_index_ray, :] = xyz[pers_index_ray, :] * depth_near[pers_index_ray, :] + origins[0].reshape(-1, 3)
    xyz_far[pers_index_ray, :] = xyz[pers_index_ray, :] * depth_far[pers_index_ray, :] + origins[0].reshape(-1, 3)

    # 3. define 3D positions for keypoints
    joints_3d = np.zeros_like(xyz)
    for i in pers_index_ray:
        if np.sum(xyz_near[i, :]) == near:
            continue

        # we use 'smplx' indices for approximating 3D joint coordinates.
        if i <= 9:  # pelvis to spine
            joints_3d[i, :] = xyz_near[i, :]
            joints_3d[i, 2] = (xyz_far[i, 2] - xyz_near[i, 2]) / 2
        elif 10 <= i <= 11:  # foot
            joints_3d[i, :] = xyz_near[i, :]
        elif 13 <= i <= 14 or 16 <= i <= 21:  # collars and body
            joints_3d[i, :] = xyz_near[i, :]
            joints_3d[i, 2] = (xyz_far[i, 2] - xyz_near[i, 2]) / 2
        elif 22 <= i <= 24:  # jaw and eyes
            joints_3d[i, :] = xyz_near[i, :]
        elif i == 55:  # nose
            joints_3d[i, :] = xyz_near[i, :]
        elif 37 <= i <= 39 or 52 <= i <= 54:  # thumbs
            joints_3d[i, :] = xyz_near[i, :]
        elif 46 <= i <= 48 or 31 <= i <= 33:  # pinky
            joints_3d[i, :] = xyz_far[i, :]
        elif i == 62 or i == 65:  # heels
            joints_3d[i, :] = xyz_far[i, :]
        elif 60 <= i <= 61 or 63 <= i <= 64:  # toes
            joints_3d[i, :] = xyz_near[i, :]
        elif 76 <= i <= 143:  # face
            joints_3d[i, :] = xyz_near[i, :]

    if visualize:
        import open3d as o3d
        mesh_o3d = o3d.geometry.PointCloud()
        mesh_o3d.points = o3d.utility.Vector3dVector(joints_3d)
        o3d.visualization.draw_geometries([mesh_o3d])

    return torch.Tensor(joints_3d[None, :, :])


def keypoint_loader(filename, pose_detector='openpose'):
    with open(filename, 'r') as f:
        pose_info = json.load(f)
        if pose_detector == 'openpose':
            if 'people' in pose_info.keys():
                keypoint = torch.concat((torch.Tensor(pose_info['people'][0]['pose_keypoints_2d']).reshape(-1, 3),
                                         torch.Tensor(pose_info['people'][0]['hand_left_keypoints_2d']).reshape(-1, 3),
                                         torch.Tensor(pose_info['people'][0]['hand_right_keypoints_2d']).reshape(-1, 3),
                                         torch.Tensor(pose_info['people'][0]['face_keypoints_2d']).reshape(-1, 3)),
                                        dim=0)
            else:
                num_human = (len(pose_info['pose_keypoints_2d']) // 63)
                if 'hand_left_keypoints_2d' not in pose_info:
                    pose_info['hand_left_keypoints_2d'] = torch.zeros((21, 3), dtype=float)
                if 'hand_right_keypoints_2d' not in pose_info:
                    pose_info['hand_right_keypoints_2d'] = torch.zeros((21, 3), dtype=float)

                if num_human == 1:
                    keypoint = torch.concat((torch.Tensor(pose_info['pose_keypoints_2d']).reshape(-1, 3),
                                             torch.Tensor(pose_info['hand_left_keypoints_2d']).reshape(-1, 3),
                                             torch.Tensor(pose_info['hand_right_keypoints_2d']).reshape(-1, 3),
                                             torch.Tensor(pose_info['face_keypoints_2d']).reshape(-1, 3)),
                                            dim=0)
                else:
                    pos = torch.mean(torch.Tensor(pose_info['pose_keypoints_2d']).reshape(-1, 25, 3), dim=1)
                    idx = np.argmin(np.abs(pos[:, 0] - 256.0))
                    keypoint = torch.concat((torch.Tensor(pose_info['pose_keypoints_2d']).reshape(-1, 25, 3),
                                             torch.Tensor(pose_info['hand_left_keypoints_2d']).reshape(-1, 21, 3),
                                             torch.Tensor(pose_info['hand_right_keypoints_2d']).reshape(-1, 21, 3),
                                             torch.Tensor(pose_info['face_keypoints_2d']).reshape(-1, 70, 3)),
                                             dim=1)
                    keypoint = keypoint[idx, :, :]
                # keypoint[25:, :] = 0

        elif pose_detector == 'openpifpaf':
            keypoint = torch.Tensor(pose_info['full_pose']).reshape(-1, 3)
        return keypoint